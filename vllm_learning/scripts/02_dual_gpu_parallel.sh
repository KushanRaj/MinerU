#!/bin/bash
# =============================================================================
# Script: 02_dual_gpu_parallel.sh
# Purpose: Dual GPU data parallel vLLM server (RECOMMENDED)
# =============================================================================

set -e  # Exit on error

echo "üöÄ Starting Dual GPU Data Parallel vLLM Server"
echo "==============================================="
echo ""
echo "üìä Configuration:"
echo "  ‚Ä¢ GPUs: 0 & 1 (data parallel)"
echo "  ‚Ä¢ Port: 30000"
echo "  ‚Ä¢ Memory Utilization: 70% per GPU"
echo "  ‚Ä¢ Max Sequences: 16 (distributed across GPUs)"
echo "  ‚Ä¢ Max Model Length: 4096 tokens"
echo ""
echo "üéì Learning Focus:"
echo "  ‚Ä¢ Data parallelism - 2 model instances"
echo "  ‚Ä¢ Load balancing across GPUs"
echo "  ‚Ä¢ Higher throughput vs single GPU"
echo ""
echo "‚ö° Expected Performance:"
echo "  ‚Ä¢ ~2x throughput compared to single GPU"
echo "  ‚Ä¢ Both GPUs should show balanced load"
echo ""
echo "üìù Monitor this server:"
echo "  ‚Ä¢ Both GPUs: watch -n 1 nvidia-smi"
echo "  ‚Ä¢ Test: mineru -p <pdf> -o outputs/test -b vlm-http-client -u http://localhost:30000"
echo "  ‚Ä¢ Benchmark: python benchmarks/benchmark_server.py"
echo ""
echo "Press Ctrl+C to stop the server"
echo "================================================"
echo ""

# Ensure we're in the right directory
cd "$(dirname "$0")/.."

# Set environment
export CUDA_VISIBLE_DEVICES=0,1  # Use both GPUs
export VLLM_LOGGING_LEVEL=INFO

# Create logs directory if it doesn't exist
mkdir -p logs

# Start server with data parallelism
mineru-vllm-server \
  --host 0.0.0.0 \
  --port 30000 \
  --data-parallel-size 2 \
  --gpu-memory-utilization 0.7 \
  --max-num-seqs 16 \
  --max-model-len 4096 \
  --enable-chunked-prefill \
  2>&1 | tee logs/dual_gpu_parallel.log



